# –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º–∏ ¬´–†–æ–∑—É–º–Ω–æ–≥–æ –º—ñ—Å—Ç–∞¬ª

## –ó–º—ñ—Å—Ç
1. [–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º–∏](#–∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞-—Å–∏—Å—Ç–µ–º–∏)
2. [–ì–∞—Ä–∞–Ω—Ç—ñ—ó –æ–±—Ä–æ–±–∫–∏](#–≥–∞—Ä–∞–Ω—Ç—ñ—ó-–æ–±—Ä–æ–±–∫–∏)
3. [–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è –∑–±–æ—ó–≤](#–≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è-–ø—ñ—Å–ª—è-–∑–±–æ—ó–≤)
4. [–í–∏–ø–∞–¥–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è](#–≤–∏–ø–∞–¥–∫–∏-–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è)

## –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º–∏

### –ó–∞–≥–∞–ª—å–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∞ —Å—Ö–µ–º–∞

```mermaid
graph TB
    subgraph "–î–∂–µ—Ä–µ–ª–∞ –¥–∞–Ω–∏—Ö"
        S1[–°–µ–Ω—Å–æ—Ä–∏ —Ç—Ä–∞—Ñ—ñ–∫—É]
        S2[–î–∞—Ç—á–∏–∫–∏ —è–∫–æ—Å—Ç—ñ –ø–æ–≤—ñ—Ç—Ä—è]
        S3[–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ñ —Å–µ–Ω—Å–æ—Ä–∏]
        S4[–ü–∞—Ä–∫—É–≤–∞–ª—å–Ω—ñ —Å–∏—Å—Ç–µ–º–∏]
        S5[Emergency —Å–µ–Ω—Å–æ—Ä–∏]
        S6[–ï–Ω–µ—Ä–≥–æ–º–æ–Ω—ñ—Ç–æ—Ä–∏]
    end
    
    subgraph "Edge Computing Layer"
        E1[Edge Gateway 1]
        E2[Edge Gateway 2]
        E3[Edge Gateway 3]
    end
    
    subgraph "Message Transport"
        K1[Kafka Cluster]
        K2[Topic: traffic-events]
        K3[Topic: air-quality]
        K4[Topic: emergency-events]
        K5[Topic: energy-data]
    end
    
    subgraph "Stream Processing"
        F1[Apache Flink Cluster]
        F2[Real-time Analytics]
        F3[Alert Engine]
        F4[Data Enrichment]
    end
    
    subgraph "Batch Processing"
        SP1[Apache Spark]
        SP2[Hourly Aggregation]
        SP3[Daily Reports]
        SP4[ML Training]
    end
    
    subgraph "Storage Layer"
        HDFS[(HDFS - Raw Data)]
        C1[(Cassandra - Time Series)]
        ES[(Elasticsearch - Search)]
        Redis[(Redis - Cache)]
    end
    
    subgraph "Services"
        API[REST API Gateway]
        WS[WebSocket Server]
        DASH[Analytics Dashboard]
        ALERT[Alert Service]
    end
    
    subgraph "External Systems"
        POLICE[Police Department]
        FIRE[Fire Department]
        TRAFFIC[Traffic Control]
        ENERGY[Energy Company]
    end
    
    S1 --> E1
    S2 --> E1
    S3 --> E2
    S4 --> E2
    S5 --> E3
    S6 --> E3
    
    E1 --> K1
    E2 --> K1
    E3 --> K1
    
    K1 --> K2
    K1 --> K3
    K1 --> K4
    K1 --> K5
    
    K2 --> F1
    K3 --> F1
    K4 --> F1
    K5 --> F1
    
    F1 --> F2
    F1 --> F3
    F1 --> F4
    
    F3 --> ALERT
    F2 --> C1
    F4 --> Redis
    
    K1 --> SP1
    SP1 --> SP2
    SP1 --> SP3
    SP1 --> SP4
    
    SP2 --> HDFS
    SP3 --> HDFS
    SP4 --> HDFS
    
    F1 --> HDFS
    C1 --> ES
    
    API --> C1
    API --> ES
    API --> Redis
    
    WS --> F3
    DASH --> API
    ALERT --> POLICE
    ALERT --> FIRE
    ALERT --> TRAFFIC
    ALERT --> ENERGY
```

### –î–µ—Ç–∞–ª—å–Ω–∏–π –æ–ø–∏—Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤

#### –î–∂–µ—Ä–µ–ª–∞ –¥–∞–Ω–∏—Ö
- –°–µ–Ω—Å–æ—Ä–∏ —Ç—Ä–∞—Ñ—ñ–∫—É - —à–≤–∏–¥–∫—ñ—Å—Ç—å, –∫—ñ–ª—å–∫—ñ—Å—Ç—å, —â—ñ–ª—å–Ω—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤
- –î–∞—Ç—á–∏–∫–∏ —è–∫–æ—Å—Ç—ñ –ø–æ–≤—ñ—Ç—Ä—è - CO‚ÇÇ, PM2.5, –æ–∑–æ–Ω —Ç–∞ —ñ–Ω—à—ñ –∑–∞–±—Ä—É–¥–Ω—é–≤–∞—á—ñ
- –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ñ —Å–µ–Ω—Å–æ—Ä–∏ - –¥–∞–Ω—ñ –≤—ñ–¥ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∏—Ö —Å–µ–Ω—Å–æ—Ä—ñ–≤ –¥–ª—è –∫–ª—ñ–º–∞—Ç–∏—á–Ω–æ–≥–æ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É. 
- –ü–∞—Ä–∫—É–≤–∞–ª—å–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ - –ø–æ–¥—ñ—ó –ø–∞—Ä–∫—É–≤–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º (–∑–∞–π–Ω—è—Ç—Ç—è/–∑–≤—ñ–ª—å–Ω–µ–Ω–Ω—è –º—ñ—Å—Ü—å).
- –ê–≤–∞—Ä—ñ–π–Ω—ñ —Å–µ–Ω—Å–æ—Ä–∏ - –ø–æ–¥—ñ—ó —Å–ø–æ—Å—Ç–µ—Ä–µ–∂—É–≤–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º –ø–æ–∂–µ–∂—ñ, –≤–∏—Ç–æ–∫–∏ –≥–∞–∑—É, –ù–°
- –ï–Ω–µ—Ä–≥–æ–º–æ–Ω—ñ—Ç–æ—Ä–∏ - –ø–æ–∫–∞–∑–Ω–∏–∫–∏ –ª—ñ—á–∏–ª—å–Ω–∏–∫—ñ–≤ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è –µ–ª–µ–∫—Ç—Ä–æ–µ–Ω–µ—Ä–≥—ñ—ó

#### 1. Edge Computing Layer
**Apache NiFi** –∞–≤—Ç–æ–º–∞—Ç–∏–∑—É—î –ø–æ—Ç–æ–∫–∏ –¥–∞–Ω–∏—Ö, —â–æ –∑–∞–±–µ–∑–ø–µ—á—É—î –∑–±—ñ—Ä, –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—é —ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–∞–Ω–∏—Ö –º—ñ–∂ —Ä—ñ–∑–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏. –ù–∞ –ø–µ—Ä–∏—Ñ–µ—Ä—ñ–π–Ω–∏—Ö —à–ª—é–∑–∞—Ö –≤–∏–∫–æ–Ω—É—î:
- –ü–µ—Ä–≤–∏–Ω–Ω—É —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—é —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
- –õ–æ–∫–∞–ª—å–Ω–µ –∫–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–∏ –≤—ñ–¥—Å—É—Ç–Ω–æ—Å—Ç—ñ –∑–≤'—è–∑–∫—É
- –ö–æ–º–ø—Ä–µ—Å—ñ—é —Ç–∞ –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –º–µ—Ä–µ–∂–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ—ñ–∫—É

**–†–æ–∑–ø–æ–¥—ñ–ª –ø–µ—Ä–∏—Ñ–µ—Ä—ñ–π–Ω–∏—Ö —à–ª—é–∑—ñ–≤:**
- **Edge Gateway 1**: –û–±—Ä–æ–±–ª—è—î –¥–∞–Ω—ñ –≤—ñ–¥ —Å–µ–Ω—Å–æ—Ä—ñ–≤ —Ç—Ä–∞—Ñ—ñ–∫—É —Ç–∞ –¥–∞—Ç—á–∏–∫—ñ–≤ —è–∫–æ—Å—Ç—ñ –ø–æ–≤—ñ—Ç—Ä—è –≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∏—Ö —Ä–∞–π–æ–Ω–∞—Ö –º—ñ—Å—Ç–∞
- **Edge Gateway 2**: –ó–±–∏—Ä–∞—î –¥–∞–Ω—ñ –≤—ñ–¥ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∏—Ö —Å–µ–Ω—Å–æ—Ä—ñ–≤ —Ç–∞ –ø–∞—Ä–∫—É–≤–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º —É –∂–∏—Ç–ª–æ–≤–∏—Ö —Ä–∞–π–æ–Ω–∞—Ö
- **Edge Gateway 3**: –í—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∑–∞ –∫—Ä–∏—Ç–∏—á–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ - –∞–≤–∞—Ä—ñ–π–Ω—ñ —Å–µ–Ω—Å–æ—Ä–∏ —Ç–∞ –µ–Ω–µ—Ä–≥–æ–º–æ–Ω—ñ—Ç–æ—Ä–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Ä–µ–∞–≥—É–≤–∞–Ω–Ω—è

#### 2. Message Transport (Apache Kafka)
**Kafka** –≤–∏—Å—Ç—É–ø–∞—î —è–∫ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞ —à–∏–Ω–∞ –ø–æ–¥—ñ–π —Å–∏—Å—Ç–µ–º–∏, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –Ω–∞–¥—ñ–π–Ω—É –¥–æ—Å—Ç–∞–≤–∫—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –º—ñ–∂ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏. –∫–æ–∂–µ–Ω —Ç–æ–ø—ñ–∫ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –ø—ñ–¥ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –ø–æ—Ç—Ä–µ–±–∏:

- **traffic-events**: –ù–∞–π–±—ñ–ª—å—à–µ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (12 —Ä–æ–∑–¥—ñ–ª—ñ–≤), –∫–æ—Ä–æ—Ç–∫–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è (7 –¥–Ω—ñ–≤)
- **air-quality**: –°–µ—Ä–µ–¥–Ω—î –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è, –¥–æ–≤—à–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è (30 –¥–Ω—ñ–≤) –¥–ª—è –µ–∫–æ–ª–æ–≥—ñ—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
- **temperature-data**: –°–µ—Ä–µ–¥–Ω—î –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è - 4 —Ä–æ–∑–¥—ñ–ª–∏ (4 –∫–≤–∞—Ä—Ç–∞–ª–∏), –ø–æ–¥–æ–≤–∂–µ–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è - 90 –¥–Ω—ñ–≤ (–¥–ª—è —Å–µ–∑–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É)
- **parking-events**: –°–µ—Ä–µ–¥–Ω—î –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è - 6 —Ä–æ–∑–¥—ñ–ª—ñ–≤, –∫–æ—Ä–æ—Ç–∫–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è - 14 –¥–Ω—ñ–≤ (—à–≤–∏–¥–∫–æ–∑–º—ñ–Ω–Ω—ñ –¥–∞–Ω—ñ)
- **emergency-events**: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ —Ä–æ–∑–¥—ñ–ª–∏ –∞–ª–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è (365 –¥–Ω—ñ–≤) –¥–ª—è –∞—É–¥–∏—Ç—É
- **energy-data**: –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è –µ–Ω–µ—Ä–≥–µ—Ç–∏—á–Ω–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤

–ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—ó = 3 –∑–∞–±–µ–∑–ø–µ—á—É—î –≤—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫—ñ—Å—Ç—å –ø—Ä–∏ –≤–∏—Ö–æ–¥—ñ –∑ –ª–∞–¥—É –¥–æ 2 –±—Ä–æ–∫–µ—Ä—ñ–≤.
```yaml
# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è Kafka —Ç–æ–ø—ñ–∫—ñ–≤
topics:
  traffic-events:
    partitions: 12
    replication-factor: 3
    retention: 7d
    
  air-quality:
    partitions: 6
    replication-factor: 3
    retention: 30d

  temperature-data:
    partitions: 4
    replication-factor: 3
    retention: 60d

  parking-events:
    partitions: 6
    replication-factor: 3
    retention: 14d
    
  emergency-events:
    partitions: 3
    replication-factor: 3
    retention: 365d
    
  energy-data:
    partitions: 8
    replication-factor: 3
    retention: 90d
```

#### 3. Stream Processing (Apache Flink)
**Apache Flink** –∑–∞–±–µ–∑–ø–µ—á—É—î –ø–æ—Ç–æ–∫–æ–≤—É –æ–±—Ä–æ–±–∫—É –¥–∞–Ω–∏—Ö —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ –∑ –Ω–∏–∑—å–∫–æ—é –∑–∞—Ç—Ä–∏–º–∫–æ—é. –†—ñ–∑–Ω—ñ –∑–∞–≤–¥–∞–Ω–Ω—è –æ–±—Ä–æ–±–ª—è—é—Ç—å —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ç–∏–ø–∏ –ø–æ–¥—ñ–π:

- **Alert Detection Job**: –í–∏—è–≤–ª—è—î –∫—Ä–∏—Ç–∏—á–Ω—ñ —Å–∏—Ç—É–∞—Ü—ñ—ó (–ø–µ—Ä–µ–≤–∏—â–µ–Ω–Ω—è CO‚ÇÇ, –∞–≤–∞—Ä—ñ—ó) —á–µ—Ä–µ–∑ CEP –ø–∞—Ç–µ—Ä–Ω–∏
- **Real-time Aggregation Job**: –û–±—á–∏—Å–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É —á–∞—Å–æ–≤–∏—Ö –≤—ñ–∫–Ω–∞—Ö (—Å–µ—Ä–µ–¥–Ω—è —à–≤–∏–¥–∫—ñ—Å—Ç—å —Ç—Ä–∞—Ñ—ñ–∫—É –∑–∞ 5 —Ö–≤–∏–ª–∏–Ω)
- **Data Enrichment Job**: –ó–±–∞–≥–∞—á—É—î –¥–∞–Ω—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é (–ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏, —á–∞—Å –¥–æ–±–∏)
- **Pattern Detection Job**: –ó–Ω–∞—Ö–æ–¥–∏—Ç—å —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω–æ—Å—Ç—ñ –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ (–∞–Ω–æ–º–∞–ª—ñ—ó –≤ –µ–Ω–µ—Ä–≥–æ—Å–ø–æ–∂–∏–≤–∞–Ω–Ω—ñ)

–§—É–Ω–∫—Ü—ñ—ó –æ–±—Ä–æ–±–∫–∏ –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –ø—ñ–¥—Ç—Ä–∏–º–∫—É —Å—Ç–∞–Ω—É, –æ–±—Ä–æ–±–∫—É –∑–∞–ø—ñ–∑–Ω—ñ–ª–∏—Ö –ø–æ–¥—ñ–π —á–µ—Ä–µ–∑ —á–∞—Å–æ–≤—ñ –º—ñ—Ç–∫–∏ (watermarks) —Ç–∞ –≤—ñ–∫–æ–Ω–Ω—ñ –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó.
```mermaid
graph LR
    subgraph "Flink Jobs"
        J1[Alert Detection Job]
        J2[Real-time Aggregation Job]
        J3[Data Enrichment Job]
        J4[Pattern Detection Job]
    end
    
    subgraph "Processing Functions"
        F1[CEP - Complex Event Processing]
        F2[Window Functions]
        F3[State Management]
        F4[Watermarks]
    end
    
    J1 --> F1
    J2 --> F2
    J3 --> F3
    J4 --> F4
```

#### 4. Batch Processing (Apache Spark)
**Apache Spark** –æ–±—Ä–æ–±–ª—è—î –≤–µ–ª–∏–∫—ñ –æ–±—Å—è–≥–∏ –¥–∞–Ω–∏—Ö —É –ø–∞–∫–µ—Ç–Ω–æ–º—É —Ä–µ–∂–∏–º—ñ –¥–ª—è –ø–æ–≥–ª–∏–±–ª–µ–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —Ç–∞ –∑–≤—ñ—Ç–Ω–æ—Å—Ç—ñ. –†—ñ–∑–Ω—ñ –∑–∞–≤–¥–∞–Ω–Ω—è –≤–∏–∫–æ–Ω—É—é—Ç—å —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó:

- **Hourly Aggregation**: –°—Ç–≤–æ—Ä—é—î –≥–æ–¥–∏–Ω–Ω—ñ –∑–≤–µ–¥–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö (—Å–µ—Ä–µ–¥–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ —Ç—Ä–∞—Ñ—ñ–∫—É, –µ–Ω–µ—Ä–≥–æ—Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è)
- **Daily Reports**: –§–æ—Ä–º—É—î —â–æ–¥–µ–Ω–Ω—ñ –∑–≤—ñ—Ç–∏ –¥–ª—è –º—ñ—Å—å–∫–∏—Ö —Å–ª—É–∂–± (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä–∫—É–≤–∞–Ω–Ω—è, —è–∫—ñ—Å—Ç—å –ø–æ–≤—ñ—Ç—Ä—è)
- **ML Training**: –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—ñ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
- **Data Quality Checks**: –í–∞–ª—ñ–¥—É—î —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö —Ç–∞ –≤–∏—è–≤–ª—è—î –∞–Ω–æ–º–∞–ª—ñ—ó –≤ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Å–∞—Ö

Spark –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ä–µ—Å—É—Ä—Å–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ –Ω—ñ—á–Ω–∏–π —á–∞—Å, –∫–æ–ª–∏ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞ –ø–æ—Ç–æ–∫–æ–≤—É –æ–±—Ä–æ–±–∫—É –º—ñ–Ω—ñ–º–∞–ª—å–Ω–µ.

#### 5. Storage Layer

**HDFS (Hadoop Distributed File System):**
- –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤–∞ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö
- –î–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –Ω–µ–æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö –≤—ñ–¥ –≤—Å—ñ—Ö —Å–µ–Ω—Å–æ—Ä—ñ–≤ –º—ñ—Å—Ç–∞
- –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –ø–æ –¥–∞—Ç–∞—Ö: `/data/year=2025/month=10/day=10/hour=14/`
- –í—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫—ñ—Å—Ç—å —á–µ—Ä–µ–∑ —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—é –±–ª–æ–∫—ñ–≤ –Ω–∞ –∫—ñ–ª—å–∫–∞ –≤—É–∑–ª—ñ–≤ (RF=3)
- –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö —Ñ–∞–π–ª—ñ–≤ Apache Spark

**Apache Cassandra:**
- NoSQL –±–∞–∑–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤ —Å–µ–Ω—Å–æ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- –í–∏—Å–æ–∫–æ–µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –∑–∞–ø–∏—Å–∏ —Ç–∞ —á–∏—Ç–∞–Ω–Ω—è –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ IoT –¥–∞–Ω–∏—Ö
- –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ —î–¥–∏–Ω–æ—ó —Ç–æ—á–∫–∏ –≤—ñ–¥–º–æ–≤–∏
- –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Ç—ñ–≤ –∑–∞ —á–∞—Å–æ–≤–∏–º–∏ –ø–µ—Ä—ñ–æ–¥–∞–º–∏ —Ç–∞ –ª–æ–∫–∞—Ü—ñ—î—é
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø–æ –≤—É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞ (RF=3)
```cql
-- –°—Ö–µ–º–∞ –¥–ª—è time-series –¥–∞–Ω–∏—Ö
CREATE TABLE sensor_data (
    sensor_id text,
    location text,
    timestamp timestamp,
    event_type text,
    value double,
    metadata map<text, text>,
    PRIMARY KEY ((sensor_id, location), timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC);
```

**Elasticsearch:**
- –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
- –ê–≥—Ä–µ–≥–∞—Ü—ñ—ó –¥–ª—è –¥–∞—à–±–æ—Ä–¥—ñ–≤
- –ì–µ–æ–ø—Ä–æ—Å—Ç–æ—Ä–æ–≤—ñ –∑–∞–ø–∏—Ç–∏

**Redis:**
- –í–∏—Å–æ–∫–æ—à–≤–∏–¥–∫—ñ—Å–Ω–∏–π –∫–µ—à –¥–ª—è —á–∞—Å—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –æ–±—á–∏—Å–ª–µ–Ω—å Flink –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
- –°–µ—Å—ñ—ó –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ —Ç–∞ —Ç–∏–º—á–∞—Å–æ–≤—ñ –¥–∞–Ω—ñ –¥–∞—à–±–æ—Ä–¥—ñ–≤
  
Redis –¥–æ–ø–æ–≤–Ω—é—î –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —è–∫ —à–≤–∏–¥–∫–∏–π –∫–µ—à –º—ñ–∂ –ø–æ—Ç–æ–∫–æ–≤–æ—é –æ–±—Ä–æ–±–∫–æ—é —Ç–∞ –∫–ª—ñ—î–Ω—Ç—Å—å–∫–∏–º–∏ –¥–æ–¥–∞—Ç–∫–∞–º–∏, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –Ω–∏–∑—å–∫—É –ª–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å –¥–ª—è —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤.

### –û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è –≤–∏–±–æ—Ä—É —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π

**Apache Kafka –ø—Ä–æ—Ç–∏ RabbitMQ/ActiveMQ:**
- –í–∏—Å–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–Ω–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å (–ø–æ–Ω–∞–¥ 1 –º—ñ–ª—å–π–æ–Ω –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –Ω–∞ —Å–µ–∫—É–Ω–¥—É)
- –î–æ–≤–≥–æ—Ç—Ä–∏–≤–∞–ª–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –ø–æ–¥—ñ–π –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –≤—ñ–¥—Ç–≤–æ—Ä–µ–Ω–Ω—è
- –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ (partitioning)
- –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ –≥–∞—Ä–∞–Ω—Ç—ñ–π –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ—ó –¥–æ—Å—Ç–∞–≤–∫–∏ (Exactly Once) –∑ Flink

**Apache Flink –ø—Ä–æ—Ç–∏ Apache Storm/Spark Streaming:**
- –°–ø—Ä–∞–≤–∂–Ω—è –ø–æ—Ç–æ–∫–æ–≤–∞ –æ–±—Ä–æ–±–∫–∞ (–Ω–µ –º—ñ–∫—Ä–æ–ø–∞–∫–µ—Ç–Ω–∞)
- –ù–∏–∑—å–∫–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ (–º–µ–Ω—à–µ 100 –º—ñ–ª—ñ—Å–µ–∫—É–Ω–¥) –∫—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è –∞–≤–∞—Ä—ñ–π–Ω–∏—Ö —Å–ø–æ–≤—ñ—â–µ–Ω—å
- –í–±—É–¥–æ–≤–∞–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ —á–∞—Å—É –ø–æ–¥—ñ–π —Ç–∞ —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫
- –ì–∞—Ä–∞–Ω—Ç—ñ—ó –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ—ó –æ–±—Ä–æ–±–∫–∏ (Exactly Once) —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω—ñ–∑–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∏—Ö —Ç–æ—á–æ–∫

**Apache Spark –ø—Ä–æ—Ç–∏ Hadoop MapReduce:**
- –£ 10-100 —Ä–∞–∑—ñ–≤ —à–≤–∏–¥—à–∞ –æ–±—Ä–æ–±–∫–∞ –∑–∞–≤–¥—è–∫–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º —É –ø–∞–º'—è—Ç—ñ
- –£–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏, –ø–æ—Ç–æ–∫—ñ–≤, –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –∑–∞–ø–∏—Ç—ñ–≤
- –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
- –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ Delta Lake –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π–Ω–∏—Ö –≥–∞—Ä–∞–Ω—Ç—ñ–π

**Cassandra –ø—Ä–æ—Ç–∏ MongoDB/PostgreSQL:**
- –õ—ñ–Ω—ñ–π–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è —ñ–Ω—Ç–µ–Ω—Å–∏–≤–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π –∑–∞–ø–∏—Å—É IoT –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤
- –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤ –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º —Ç–µ—Ä–º—ñ–Ω–æ–º –∂–∏—Ç—Ç—è
- –î–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ —î–¥–∏–Ω–æ—ó —Ç–æ—á–∫–∏ –≤—ñ–¥–º–æ–≤–∏
- –í–∏—Å–æ–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å —É –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏—Ö —Ü–µ–Ω—Ç—Ä–∞—Ö –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö

**HDFS –ø—Ä–æ—Ç–∏ Amazon S3/Azure Blob:**
- –õ–æ–∫–∞–ª—å–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –¥–ª—è –≤–∏–º–æ–≥ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ –º—ñ—Å—å–∫–æ—ó –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ü—ñ—ó
- –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ –µ–∫–æ—Å–∏—Å—Ç–µ–º–æ—é Hadoop (Spark, Hive)
- –ù–∏–∂—á–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
- –ö–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –¥–∞–Ω–∏–º–∏ –±–µ–∑ –ø—Ä–∏–≤'—è–∑–∫–∏ –¥–æ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞

**Redis –ø—Ä–æ—Ç–∏ Memcached/Hazelcast:**
- –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–∏—Ö (–≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ –º–Ω–æ–∂–∏–Ω–∏, —Ö–µ—à-—Ç–∞–±–ª–∏—Ü—ñ)
- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –Ω–∞ –¥–∏—Å–∫—É —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω—ñ–∑–º–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è
- –ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó/–ø—ñ–¥–ø–∏—Å–∫–∏ –¥–ª—è —Å–ø–æ–≤—ñ—â–µ–Ω—å —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
- –°–∫—Ä–∏–ø—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π

### –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —Ç–∞ –≤—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫—ñ—Å—Ç—å

#### –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
```mermaid
graph TB
    subgraph "Kafka Cluster"
        K1[Broker 1]
        K2[Broker 2]
        K3[Broker 3]
        K4[Broker 4]
        K5[Broker 5]
    end
    
    subgraph "Flink Cluster"
        F1[JobManager 1]
        F2[JobManager 2 - Standby]
        F3[TaskManager 1]
        F4[TaskManager 2]
        F5[TaskManager 3]
        F6[TaskManager 4]
    end
    
    subgraph "Cassandra Ring"
        C1[Node 1 - DC1]
        C2[Node 2 - DC1]
        C3[Node 3 - DC1]
        C4[Node 1 - DC2]
        C5[Node 2 - DC2]
        C6[Node 3 - DC2]
    end
```

#### –ú–µ—Ö–∞–Ω—ñ–∑–º–∏ –≤—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫–æ—Å—Ç—ñ
1. **Kafka**: –ú—ñ–Ω. 3 —Ä–µ–ø–ª—ñ–∫–∏, ISR (In-Sync Replicas)
2. **Flink**: Checkpointing –∫–æ–∂–Ω—ñ 30 —Å–µ–∫—É–Ω–¥
3. **Cassandra**: RF=3, Consistency Level=QUORUM
4. **HDFS**: Replication factor = 3

## –ì–∞—Ä–∞–Ω—Ç—ñ—ó –æ–±—Ä–æ–±–∫–∏

### –ú–∞—Ç—Ä–∏—Ü—è –≥–∞—Ä–∞–Ω—Ç—ñ–π –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –ì–∞—Ä–∞–Ω—Ç—ñ—è | –û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è |
|-----------|----------|---------------|
| **Edge ‚Üí Kafka** | At Least Once | Retry –º–µ—Ö–∞–Ω—ñ–∑–º, idempotent producer |
| **Kafka ‚Üí Flink (Alerts)** | Exactly Once | Kafka transactions + Flink checkpoints |
| **Kafka ‚Üí Flink (Analytics)** | At Least Once | –î–æ–ø—É—Å—Ç–∏–º—ñ –¥—É–±–ª–∏ –≤ –∞–Ω–∞–ª—ñ—Ç–∏—Ü—ñ |
| **Flink ‚Üí Cassandra** | At Least Once | Upsert –æ–ø–µ—Ä–∞—Ü—ñ—ó, idempotent writes |
| **Spark Batch Jobs** | Exactly Once | Delta Lake ACID transactions |
| **Alert Notifications** | Exactly Once | Deduplication –ø–æ message ID |

### Exactly Once –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—ñ–¥—Å–∏—Å—Ç–µ–º

#### 1. Emergency Alert System
```java
// Flink job –∑ exactly-once —Å–µ–º–∞–Ω—Ç–∏–∫–æ—é
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.enableCheckpointing(30000); // 30 —Å–µ–∫—É–Ω–¥
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// Kafka source –∑ exactly-once
FlinkKafkaConsumer<EmergencyEvent> consumer = new FlinkKafkaConsumer<>(
    "emergency-events",
    new EmergencyEventSchema(),
    kafkaProps
);
consumer.setStartFromEarliest();

DataStream<Alert> alerts = env
    .addSource(consumer)
    .keyBy(event -> event.getLocation())
    .process(new EmergencyDetectionFunction())
    .addSink(new ExactlyOnceAlertSink());
```

#### 2. Deduplication –º–µ—Ö–∞–Ω—ñ–∑–º
```java
// –°—Ç–∞–Ω –¥–ª—è tracking processed events
ValueState<Set<String>> processedEvents = getRuntimeContext()
    .getState(new ValueStateDescriptor<>("processed", Set.class));

public void processElement(EmergencyEvent event, Context ctx, Collector<Alert> out) {
    Set<String> processed = processedEvents.value();
    if (processed == null) processed = new HashSet<>();
    
    String eventId = event.getId();
    if (!processed.contains(eventId)) {
        // Process only if not seen before
        Alert alert = processEmergencyEvent(event);
        out.collect(alert);
        
        processed.add(eventId);
        processedEvents.update(processed);
    }
}
```

### Idempotency patterns

#### Cassandra Upserts
```cql
-- Idempotent insert with timestamp
INSERT INTO sensor_readings (sensor_id, timestamp, value, processed_at)
VALUES (?, ?, ?, toTimestamp(now()))
IF NOT EXISTS;

-- Or use UPDATE with conditions
UPDATE sensor_readings 
SET value = ?, last_updated = toTimestamp(now())
WHERE sensor_id = ? AND timestamp = ?
IF last_updated < ?;
```

## –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è –∑–±–æ—ó–≤

### –°—Ü–µ–Ω–∞—Ä—ñ—ó –≤—ñ–¥–º–æ–≤ —Ç–∞ recovery —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó

#### 1. –í—ñ–¥–º–æ–≤–∞ Kafka Broker

```mermaid
sequenceDiagram
    participant P as Producer
    participant B1 as Broker 1 (Leader)
    participant B2 as Broker 2 (Follower)
    participant B3 as Broker 3 (Follower)
    participant C as Consumer
    
    P->>B1: Send message
    B1->>B2: Replicate
    B1->>B3: Replicate
    B1-->>P: Ack
    
    Note over B1: Broker 1 fails
    
    B2->>B3: Leader election
    Note over B2: B2 becomes leader
    
    P->>B2: Send message (auto-retry)
    B2->>B3: Replicate
    B2-->>P: Ack
    
    C->>B2: Consume (continues seamlessly)
```

**Recovery –º–µ—Ö–∞–Ω—ñ–∑–º:**
- Automatic leader election —Å–µ—Ä–µ–¥ ISR
- Producers –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ retry –∑ –Ω–æ–≤–∏–º –ª—ñ–¥–µ—Ä–æ–º
- Consumers –ø—Ä–æ–¥–æ–≤–∂—É—é—Ç—å —á–∏—Ç–∞–Ω–Ω—è –∑ –Ω–æ–≤–æ–≥–æ –ª—ñ–¥–µ—Ä–∞
- –í—ñ–¥–Ω–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ —Ä–µ–ø–ª—ñ–∫—É—é—Ç—å—Å—è –Ω–∞ –Ω–æ–≤–∏–π broker

#### 2. –í—ñ–¥–º–æ–≤–∞ Flink JobManager

```yaml
# Flink HA –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
high-availability: zookeeper
high-availability.zookeeper.quorum: zk1:2181,zk2:2181,zk3:2181
high-availability.zookeeper.path.root: /flink
high-availability.cluster-id: smart-city-cluster

# Checkpointing
state.checkpoints.dir: hdfs://namenode/flink-checkpoints
state.savepoints.dir: hdfs://namenode/flink-savepoints
```

**Recovery –ø—Ä–æ—Ü–µ—Å:**
1. Standby JobManager —Å—Ç–∞—î –∞–∫—Ç–∏–≤–Ω–∏–º
2. –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞–Ω—É –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ checkpoint
3. Restart failed tasks –∑ checkpoint –ø–æ–∑–∏—Ü—ñ—ó
4. Consumers –ø—Ä–æ–¥–æ–≤–∂—É—é—Ç—å –∑ –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ offset

#### 3. –í—ñ–¥–º–æ–≤–∞ HDFS DataNode

```mermaid
graph TB
    subgraph "Before Failure"
        NN1[NameNode]
        DN1[DataNode 1]
        DN2[DataNode 2]
        DN3[DataNode 3]
        
        NN1 --> DN1
        NN1 --> DN2
        NN1 --> DN3
        
        DN1 -.->|Block A rep1| B1[Block A]
        DN2 -.->|Block A rep2| B2[Block A]
        DN3 -.->|Block A rep3| B3[Block A]
    end
    
    subgraph "After DN2 Failure"
        NN2[NameNode]
        DN1_2[DataNode 1]
        DN3_2[DataNode 3]
        DN4[DataNode 4 - New]
        
        NN2 --> DN1_2
        NN2 --> DN3_2
        NN2 --> DN4
        
        DN1_2 -.->|Block A rep1| B1_2[Block A]
        DN3_2 -.->|Block A rep2| B3_2[Block A]
        DN4 -.->|Block A rep3| B4[Block A - Replicated]
    end
```

**Recovery steps:**
1. NameNode detects DataNode failure
2. Marks blocks as under-replicated
3. Schedules re-replication to healthy nodes
4. Updates block locations in metadata

### –ö–æ–Ω—Ç—Ä–æ–ª—å —Ü—ñ–ª—ñ—Å–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö

#### Checksums —Ç–∞ validation
```java
// Kafka producer –∑ checksum validation
Properties props = new Properties();
props.put("enable.idempotence", "true");
props.put("acks", "all");
props.put("retries", Integer.MAX_VALUE);
props.put("max.in.flight.requests.per.connection", 5);

// Custom serializer –∑ checksum
public class ChecksumAvroSerializer extends AbstractKafkaAvroSerializer {
    @Override
    public byte[] serialize(String topic, Object record) {
        byte[] data = super.serialize(topic, record);
        // Add CRC32 checksum
        CRC32 checksum = new CRC32();
        checksum.update(data);
        
        ByteBuffer buffer = ByteBuffer.allocate(data.length + 4);
        buffer.put(data);
        buffer.putInt((int) checksum.getValue());
        return buffer.array();
    }
}
```

#### Data lineage tracking
```sql
-- –ú–µ—Ç–∞–¥–∞–Ω—ñ –¥–ª—è tracking data flow
CREATE TABLE data_lineage (
    job_id TEXT,
    input_topic TEXT,
    output_table TEXT,
    processing_time TIMESTAMP,
    record_count BIGINT,
    checksum TEXT,
    PRIMARY KEY (job_id, processing_time)
);
```

## –í–∏–ø–∞–¥–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

### 1. Stream Processing: Real-time CO‚ÇÇ Alert Detection

#### Flink SQL –∑–∞–ø–∏—Ç
```sql
-- –í–∏—è–≤–ª–µ–Ω–Ω—è –ø–µ—Ä–µ–≤–∏—â–µ–Ω–Ω—è —Ä—ñ–≤–Ω—è CO‚ÇÇ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
CREATE TABLE air_quality_stream (
    sensor_id STRING,
    location STRING,
    timestamp TIMESTAMP(3),
    co2_level DOUBLE,
    temperature DOUBLE,
    humidity DOUBLE,
    WATERMARK FOR timestamp AS timestamp - INTERVAL '10' SECONDS
) WITH (
    'connector' = 'kafka',
    'topic' = 'air-quality',
    'properties.bootstrap.servers' = 'kafka:9092',
    'format' = 'avro'
);

-- CEP pattern –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è —Å—Ç—ñ–π–∫–∏—Ö –ø–µ—Ä–µ–≤–∏—â–µ–Ω—å
CREATE TABLE co2_alerts AS
SELECT 
    location,
    AVG(co2_level) as avg_co2,
    MAX(co2_level) as max_co2,
    COUNT(*) as readings_count,
    TUMBLE_START(timestamp, INTERVAL '5' MINUTES) as window_start,
    TUMBLE_END(timestamp, INTERVAL '5' MINUTES) as window_end
FROM air_quality_stream
WHERE co2_level > 800 -- –ö—Ä–∏—Ç–∏—á–Ω–∏–π —Ä—ñ–≤–µ–Ω—å CO‚ÇÇ
GROUP BY 
    location,
    TUMBLE(timestamp, INTERVAL '5' MINUTES)
HAVING COUNT(*) >= 3; -- –ú—ñ–Ω—ñ–º—É–º 3 –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –≤ –≤—ñ–∫–Ω—ñ
```

#### Flink Java implementation
```java
// CEP –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è patterns
Pattern<AirQualityEvent, ?> alertPattern = Pattern
    .<AirQualityEvent>begin("high_co2")
    .where(SimpleCondition.of(event -> event.getCo2Level() > 800))
    .times(3)
    .within(Time.minutes(5));

// Pattern detection
PatternStream<AirQualityEvent> patternStream = CEP.pattern(
    airQualityStream.keyBy(AirQualityEvent::getLocation),
    alertPattern
);

// Generate alerts
DataStream<Co2Alert> alerts = patternStream.process(
    new PatternProcessFunction<AirQualityEvent, Co2Alert>() {
        @Override
        public void processMatch(
            Map<String, List<AirQualityEvent>> pattern,
            Context ctx,
            Collector<Co2Alert> out
        ) {
            List<AirQualityEvent> events = pattern.get("high_co2");
            double avgCo2 = events.stream()
                .mapToDouble(AirQualityEvent::getCo2Level)
                .average()
                .orElse(0.0);
                
            Co2Alert alert = new Co2Alert(
                events.get(0).getLocation(),
                avgCo2,
                ctx.timestamp(),
                AlertSeverity.HIGH
            );
            
            out.collect(alert);
        }
    }
);
```

**–ß–æ–º—É Stream Processing –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π:**
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å**: Alert –∑–∞ 5-10 —Å–µ–∫—É–Ω–¥ –ø—ñ—Å–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è
- **Continuous monitoring**: –ë–µ–∑–ø–µ—Ä–µ—Ä–≤–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö
- **Stateful processing**: Tracking trends —Ç–∞ patterns
- **Low latency**: Critical –¥–ª—è emergency response

### 2. Batch Processing: –ú—ñ—Å—è—á–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —à—É–º—É –ø–æ —Ä–∞–π–æ–Ω–∞—Ö

#### Spark SQL –∑–∞–ø–∏—Ç
```sql
-- –ü–∞–∫–µ—Ç–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —à—É–º–æ–≤–æ–≥–æ –∑–∞–±—Ä—É–¥–Ω–µ–Ω–Ω—è
WITH hourly_noise AS (
    SELECT 
        location,
        district,
        date_trunc('hour', timestamp) as hour,
        AVG(noise_level) as avg_noise,
        MAX(noise_level) as max_noise,
        MIN(noise_level) as min_noise,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY noise_level) as median_noise
    FROM noise_sensors_data
    WHERE timestamp >= '2025-09-01' 
      AND timestamp < '2025-10-01'
    GROUP BY location, district, date_trunc('hour', timestamp)
),
daily_noise AS (
    SELECT 
        district,
        date_trunc('day', hour) as day,
        AVG(avg_noise) as daily_avg_noise,
        MAX(max_noise) as daily_max_noise,
        MIN(min_noise) as daily_min_noise,
        AVG(median_noise) as daily_median_noise
    FROM hourly_noise
    GROUP BY district, date_trunc('day', hour)
)
SELECT 
    district,
    AVG(daily_avg_noise) as monthly_avg_noise,
    MAX(daily_max_noise) as monthly_max_noise,
    MIN(daily_min_noise) as monthly_min_noise,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY daily_avg_noise) as p95_noise,
    STDDEV(daily_avg_noise) as noise_std_deviation,
    COUNT(DISTINCT day) as days_with_data,
    -- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è —Ä–∞–π–æ–Ω—ñ–≤ –∑–∞ —Ä—ñ–≤–Ω–µ–º —à—É–º—É
    CASE 
        WHEN AVG(daily_avg_noise) > 70 THEN 'HIGH_NOISE'
        WHEN AVG(daily_avg_noise) > 55 THEN 'MODERATE_NOISE'
        ELSE 'LOW_NOISE'
    END as noise_category,
    -- Compliance –∑ –Ω–æ—Ä–º–∞–º–∏ –Ñ–°
    SUM(CASE WHEN daily_avg_noise > 55 THEN 1 ELSE 0 END) / COUNT(*) * 100 as pct_exceeding_eu_limit
FROM daily_noise
GROUP BY district
ORDER BY monthly_avg_noise DESC;
```

#### Spark DataFrame implementation
```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

// –ß–∏—Ç–∞–Ω–Ω—è –ø–∞—Ä—Ç–∏—Ü—ñ–æ–Ω–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
val noiseData = spark.read
  .option("basePath", "hdfs://namenode/data/noise_sensors/")
  .parquet("hdfs://namenode/data/noise_sensors/year=2025/month=09/*")

// Window functions –¥–ª—è temporal analysis
val hourlyWindow = Window
  .partitionBy("location", "district")
  .orderBy("timestamp")
  .rangeBetween(-3600, 3600) // 1 –≥–æ–¥–∏–Ω–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

val enrichedData = noiseData
  .withColumn("hour", date_trunc("hour", col("timestamp")))
  .withColumn("moving_avg", avg("noise_level").over(hourlyWindow))
  .withColumn("noise_trend", 
    col("noise_level") - lag("noise_level", 1).over(hourlyWindow))

// –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –ø–æ —Ä–∞–π–æ–Ω–∞—Ö
val districtStats = enrichedData
  .groupBy("district")
  .agg(
    avg("noise_level").as("avg_noise"),
    max("noise_level").as("max_noise"),
    min("noise_level").as("min_noise"),
    stddev("noise_level").as("std_noise"),
    expr("percentile_approx(noise_level, 0.95)").as("p95_noise"),
    countDistinct("location").as("sensor_count"),
    // –ß–∞—Å–æ–≤—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏
    avg(when(hour(col("timestamp")).between(7, 19), col("noise_level"))).as("day_avg"),
    avg(when(hour(col("timestamp")).between(20, 6), col("noise_level"))).as("night_avg")
  )

// –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
districtStats
  .coalesce(1)
  .write
  .mode("overwrite")
  .option("path", "hdfs://namenode/reports/monthly_noise_stats/2025/09")
  .saveAsTable("monthly_noise_report")
```

**–ß–æ–º—É Batch Processing –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π:**
- **–í–µ–ª–∏–∫—ñ –æ–±—Å—è–≥–∏**: –ê–Ω–∞–ª—ñ–∑ –º—ñ–ª—å–π–æ–Ω—ñ–≤ –∑–∞–ø–∏—Å—ñ–≤ –∑–∞ –º—ñ—Å—è—Ü—å
- **–°–∫–ª–∞–¥–Ω—ñ –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó**: Multi-level grouping, percentiles, statistical functions
- **Historical analysis**: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ –ø–µ—Ä—ñ–æ–¥–∞–º–∏
- **Resource efficiency**: –û–ø—Ç–∏–º–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ off-peak hours
- **Data consistency**: –ü–æ–≤–Ω–∏–π dataset –¥–ª—è —Ç–æ—á–Ω–∏—Ö —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—ñ–≤

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Stream/Batch

```mermaid
graph TB
    subgraph "Lambda Architecture"
        subgraph "Speed Layer (Stream)"
            SL1[Real-time Alerts]
            SL2[Live Dashboards]
            SL3[Immediate Response]
        end
        
        subgraph "Batch Layer"
            BL1[Historical Analysis]
            BL2[Monthly Reports]
            BL3[ML Model Training]
            BL4[Data Quality Checks]
        end
        
        subgraph "Serving Layer"
            API[Unified API]
            CACHE[Result Cache]
            MERGE[Data Reconciliation]
        end
    end
    
    SL1 --> API
    SL2 --> API
    BL1 --> API
    BL2 --> API
    API --> CACHE
    BL4 --> MERGE
    SL3 --> MERGE
```

**–ü–µ—Ä–µ–≤–∞–≥–∏ hybrid –ø—ñ–¥—Ö–æ–¥—É:**
1. **Latency optimization**: Stream –¥–ª—è real-time, batch –¥–ª—è accuracy
2. **Resource utilization**: Stream processing 24/7, batch –≤ –Ω—ñ—á–Ω–∏–π —á–∞—Å
3. **Fault tolerance**: Batch layer —è–∫ backup –¥–ª—è stream results
4. **Flexibility**: –†—ñ–∑–Ω—ñ SLA –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö use cases
5. **Cost optimization**: Spot instances –¥–ª—è batch jobs

## –í–∏—Å–Ω–æ–≤–∫–∏

–ó–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞–±–µ–∑–ø–µ—á—É—î:

### üéØ **–ö–ª—é—á–æ–≤—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏**
- **Scalability**: –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- **Fault tolerance**: Multi-layer redundancy —Ç–∞ automatic recovery
- **Low latency**: <1 —Å–µ–∫—É–Ω–¥–∞ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö alerts
- **High throughput**: >1M events/second
- **Data consistency**: Exactly-once –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π

### üìä **Metrics —Ç–∞ SLA**
- **Alert response time**: <5 —Å–µ–∫—É–Ω–¥ –¥–ª—è emergency events
- **System availability**: 99.9% uptime
- **Data retention**: 7 —Ä–æ–∫—ñ–≤ –¥–ª—è compliance
- **Recovery time**: <30 —Å–µ–∫—É–Ω–¥ –¥–ª—è component failures

### üîÑ **–û–ø–µ—Ä–∞—Ü—ñ–π–Ω—ñ –ø–µ—Ä–µ–≤–∞–≥–∏**
- **Unified monitoring**: Prometheus + Grafana –¥–ª—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- **Automated scaling**: Kubernetes HPA –Ω–∞ –æ—Å–Ω–æ–≤—ñ metrics
- **Data governance**: Schema registry —Ç–∞ data lineage tracking
- **Cost optimization**: Intelligent data tiering (hot/warm/cold storage)

–¶—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞–±–µ–∑–ø–µ—á—É—î –Ω–∞–¥—ñ–π–Ω—É –æ—Å–Ω–æ–≤—É –¥–ª—è —Å–∏—Å—Ç–µ–º–∏ —Ä–æ–∑—É–º–Ω–æ–≥–æ –º—ñ—Å—Ç–∞ –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é –µ–≤–æ–ª—é—Ü—ñ—ó —Ç–∞ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ.
